{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a8ec66d-c5c9-4687-9f3f-47e04957c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice\n"
     ]
    }
   ],
   "source": [
    "import merlin.models.tf as mm\n",
    "print('nice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4926252-42cf-4eac-a796-30cc7ee38829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  feed_id favourite_team favourite_club  like_count  \\\n",
      "233877   150898  1526779             25            646           3   \n",
      "142779  1722083  2232154             25            646           5   \n",
      "63693   1258962  3001679             25        unknown          26   \n",
      "149738  1491144  2190147             25            646           8   \n",
      "132707   212590  2279450             25         123216           7   \n",
      "\n",
      "        comment_count  share_count  player_id  match_id     cid media_type  \\\n",
      "233877              0            0        620     87016  129438       text   \n",
      "142779              0            0        638     87735  129413       text   \n",
      "63693               7            1      93855     87750  129413       text   \n",
      "149738              0            0      46124     87732  129413       text   \n",
      "132707              0            0        123     87737  129413       text   \n",
      "\n",
      "        hour_of_day  day_of_week  is_weekend  recency_seconds  \n",
      "233877            8            6           1     22034.233272  \n",
      "142779           12            5           1     14469.438905  \n",
      "63693            12            4           0      2931.026213  \n",
      "149738           11            3           0       616.962158  \n",
      "132707           17            6           1     40213.603033  \n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "def clean_multihot(val):\n",
    "    if pd.isna(val) or val in [\"{}\", \"[]\", \"unknown\"]:\n",
    "        return \"\"\n",
    "    if isinstance(val, float):\n",
    "        return \"\"\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip()\n",
    "        if val.startswith(\"{\") or val.startswith(\"[\"):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(val)\n",
    "                return \"|\".join(str(x) for x in parsed)\n",
    "            except:\n",
    "                return val.replace(\"{\", \"\").replace(\"}\", \"\").replace(\",\", \"|\").replace(\" \", \"\")\n",
    "        else:\n",
    "            return str(val)\n",
    "    elif isinstance(val, list):\n",
    "        return \"|\".join(str(x) for x in val)\n",
    "    else:\n",
    "        return str(val)\n",
    "\n",
    "df = pd.read_csv(\"/workspace/data/src/data/user_feed_data_set.csv\", low_memory=False)\n",
    "\n",
    "selected_fields = ['user_id', 'feed_id', 'favourite_team', 'favourite_club', 'like_count', 'comment_count', 'share_count', 'player_id', 'match_id', 'cid', 'media_type','hour_of_day', 'day_of_week', 'is_weekend', 'recency_seconds']\n",
    "\n",
    "sample_df = df[selected_fields].sample(frac=0.1, random_state=42)\n",
    "samplehead = sample_df.head()\n",
    "print(samplehead)\n",
    "\n",
    "\n",
    "sample_df.to_csv('./sample_user_feed_data_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c559be1-7d59-41cc-90ba-ee88b6dd3c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import nvtabular as nvt\n",
    "import pandas as pd\n",
    "import os\n",
    "from nvtabular.ops import Categorify, Normalize, TagAsUserID, TagAsItemID, TagAsUserFeatures, TagAsItemFeatures, LambdaOp\n",
    "\n",
    "# Define the schema for the dataset\n",
    "# user_ops = {\n",
    "#     \"user_id\": [TagAsUserID(), Categorify()],\n",
    "#     \"favourite_team\": [TagAsUserFeatures(), Categorify()],\n",
    "#     \"favourite_club\": [TagAsUserFeatures(), Categorify()],\n",
    "#     \"favourite_players\": [TagAsUserFeatures(), nvt.ops.LambdaOp(lambda col: col.str.split(\"|\")), Categorify()],\n",
    "#     \"following\": [TagAsUserFeatures(), nvt.ops.LambdaOp(lambda col: col.str.split(\"|\")), Categorify()],\n",
    "#     \"hour_of_day\": [TagAsUserFeatures(), Categorify()],\n",
    "#     \"day_of_week\": [TagAsUserFeatures(), Categorify()],\n",
    "#     \"is_weekend\": [TagAsUserFeatures(), Categorify()],\n",
    "#     \"recency_seconds\": [TagAsUserFeatures(), Normalize()]\n",
    "# }\n",
    "\n",
    "# # ------------------------\n",
    "# # ITEM FEATURES\n",
    "# # ------------------------\n",
    "# item_ops = {\n",
    "#     \"feed_id\": [TagAsItemID(), Categorify()],\n",
    "#     \"player_id\": [TagAsItemFeatures(), Categorify()],\n",
    "#     \"match_id\": [TagAsItemFeatures(), Categorify()],\n",
    "#     \"cid\": [TagAsItemFeatures(), Categorify()],\n",
    "#     \"media_type\": [TagAsItemFeatures(), Categorify()],\n",
    "#     \"tags\": [TagAsItemFeatures(), nvt.ops.LambdaOp(lambda col: col.str.split(\"|\")), Categorify()],\n",
    "#     \"like_count\": [TagAsItemFeatures(), Normalize()],\n",
    "#     \"comment_count\": [TagAsItemFeatures(), Normalize()],\n",
    "#     \"share_count\": [TagAsItemFeatures(), Normalize()]\n",
    "\n",
    "# }\n",
    "user_ops = (\n",
    "    ([\"user_id\"] >> TagAsUserID() >> Categorify()) +\n",
    "    ([\"favourite_team\", \"favourite_club\"] >> TagAsUserFeatures() >> Categorify()) +\n",
    "    ([\"hour_of_day\", \"day_of_week\", \"is_weekend\"] >> TagAsUserFeatures() >> Categorify()) +\n",
    "    ([\"recency_seconds\"] >> TagAsUserFeatures() >> Normalize())\n",
    ")\n",
    "\n",
    "item_ops = (\n",
    "    ([\"feed_id\"] >> TagAsItemID() >> Categorify()) +\n",
    "    ([\"player_id\", \"match_id\", \"cid\", \"media_type\"] >> TagAsItemFeatures() >> Categorify()) +\n",
    "    ([\"like_count\", \"comment_count\", \"share_count\"] >> TagAsItemFeatures() >> Normalize())\n",
    ")\n",
    "sample_df = pd.read_csv('./sample_user_feed_data_set.csv')\n",
    "workflow = nvt.Workflow(\n",
    "    user_ops + item_ops\n",
    ")\n",
    "\n",
    "sample_df['interactions'] = 1  # Implicit feedback - assuming all examples are positive interactions\n",
    "\n",
    "# Create a dataset for NVTabular\n",
    "dataset = nvt.Dataset(sample_df)\n",
    "\n",
    "output_path = './data/processed'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Fit and transform the data using the workflow\n",
    "workflow.fit_transform(dataset)\n",
    "\n",
    "# Save the workflow for later use (e.g., inference)\n",
    "workflow.save(os.path.join(output_path, 'workflow'))\n",
    "\n",
    "\n",
    "workflow.transform(dataset).to_parquet(\n",
    "    output_path=os.path.join(output_path, 'transformed_data')\n",
    ")\n",
    "\n",
    "# Split the data into train and validation sets (80/20 split)\n",
    "train, valid = nvt.Dataset(os.path.join(output_path, 'transformed_data/*.parquet')).to_ddf().random_split(\n",
    "    [0.8, 0.2], random_state=42\n",
    ")\n",
    "\n",
    "# Save train and validation splits\n",
    "train.to_parquet(os.path.join(output_path, 'train'))\n",
    "valid.to_parquet(os.path.join(output_path, 'valid'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a21980b-ee22-42c2-92fb-fdf1d5cca713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 09:15:17.498598: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/usr/local/lib/python3.10/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnvtabular\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnvt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m schema \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39moutput_schema\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/models/tf/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2021, NVIDIA CORPORATION.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Must happen before any importing of tensorflow to curtail mem usage\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configure_tensorflow\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindex\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexBlock, TopKIndexBlock\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmerlin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtabular\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsTabular, Filter, TabularBlock\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/merlin/dataloader/tf_utils.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column_v2 \u001b[38;5;28;01mas\u001b[39;00m fc\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:432\u001b[0m\n\u001b[1;32m    430\u001b[0m _kernel_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_tf_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_kernel_dir):\n\u001b[0;32m--> 432\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_kernel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Load third party dynamic kernels.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _s \u001b[38;5;129;01min\u001b[39;00m _site_packages_dirs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /usr/local/lib/python3.10/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE"
     ]
    }
   ],
   "source": [
    "\n",
    "import merlin\n",
    "import merlin.models.tf as mm\n",
    "import nvtabular as nvt\n",
    "\n",
    "schema = workflow.output_schema\n",
    "\n",
    "# Load the processed train and validation datasets\n",
    "train = merlin.io.Dataset(\"./data/processed/train/*.parquet\", schema=schema)\n",
    "valid = merlin.io.Dataset(\"./data/processed/valid/*.parquet\", schema=schema)\n",
    "\n",
    "# Build the Two-Tower retrieval model\n",
    "model = mm.TwoTowerModel(\n",
    "    train.schema,\n",
    "    query_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    item_tower=mm.MLPBlock([128, 64], no_activation_last_layer=True),\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    "    samplers=[mm.InBatchSampler()]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    run_eagerly=False,\n",
    "    metrics=[mm.RecallAt(10), mm.NDCGAt(10)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    batch_size=4096,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"./data/processed/two_tower_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bead43-e0ec-4ac1-bcf2-179a6632d5a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/usr/local/lib/python3.10/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py:432\u001b[0m\n\u001b[1;32m    430\u001b[0m _kernel_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_tf_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_kernel_dir):\n\u001b[0;32m--> 432\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_kernel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Load third party dynamic kernels.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _s \u001b[38;5;129;01min\u001b[39;00m _site_packages_dirs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /usr/local/lib/python3.10/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow15OpKernelContext21CtxFailureWithWarningEPKciRKN3tsl6StatusE"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the saved TwoTowerModel\n",
    "loaded_model = tf.keras.models.load_model(\"./data/processed/two_tower_model\")\n",
    "_ = loaded_model.predict(valid, batch_size=1024, steps=1)\n",
    "item_embeddings = loaded_model.item_embeddings(valid, batch_size=1024)\n",
    "user_embeddings = loaded_model.query_embeddings(valid, batch_size=1024)\n",
    "df_user = user_embeddings.to_ddf().compute()\n",
    "df_item = item_embeddings.to_ddf().compute()\n",
    "\n",
    "user_vocab_df = pd.read_parquet(\"./data/processed/workflow/categories/unique.user_id.parquet\")\n",
    "feed_vocab_df = pd.read_parquet(\"./data/processed/workflow/categories/unique.feed_id.parquet\")\n",
    "\n",
    "encoded_to_user = dict(enumerate(user_vocab_df[\"user_id\"].values))\n",
    "encoded_to_feed = dict(enumerate(feed_vocab_df[\"feed_id\"].values))\n",
    "\n",
    "# --- Add original IDs back to dataframes ---\n",
    "df_user[\"original_user_id\"] = df_user[\"user_id\"].map(encoded_to_user)\n",
    "df_item[\"original_feed_id\"] = df_item[\"feed_id\"].map(encoded_to_feed)\n",
    "vector_columns = [str(i) for i in range(64)] \n",
    "user_vector = df_user[df_user[\"user_id\"] == 11][vector_columns].iloc[0].values.astype(\"float32\")\n",
    "item_vectors = df_item[vector_columns].values.astype(\"float32\")\n",
    "scores = item_vectors @ user_vector\n",
    "\n",
    "k = 100\n",
    "top_k_idx = np.argsort(scores)[-min(k, len(scores)):][::-1]\n",
    "top_k_items = df_item.iloc[top_k_idx]\n",
    "\n",
    "print(top_k_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d6f109-b019-45b7-be57-0021d3a55fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_FOLDER = os.environ.get(\"DATA_FOLDER\", \"/workspace/data/\")\n",
    "print(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf53bb2-82f7-428d-a0b6-08be50c88944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
